{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import copy\n",
    "\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/users/jamesthomas/MachineLearning_Python/Lab3/data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>30', 'NO', '<30'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['readmitted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "X = train.drop(['readmitted'], axis=1)\n",
    "y = (train['readmitted'] == 'NO').astype(np.int).values # make problem binary\n",
    "\n",
    "X_test = test.drop(['readmitted'], axis=1)\n",
    "y_test = (test['readmitted'] == 'NO').astype(np.int).values # make problem binary\n",
    "\n",
    "#ds = load_iris()\n",
    "#X = ds.data\n",
    "#y = (ds.target>1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 28 µs, total: 40 µs\n",
      "Wall time: 43.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class BinaryLogisticRegression:\n",
    "    '''\n",
    "    method: 'default', 'LineSearchLogisticRegression', 'StochasticLogisticRegression', \n",
    "    'MiniBatchStochasticLogisticRegression','HessianBinaryLogisticRegression', 'BFGSBinaryLogisticRegression'\n",
    "    regularization_metric: 'NA', 'L1', 'L2'\n",
    "    batch_size: float between 0 to 1, only valid when call 'MiniBatchStochasticLogisticRegression' method\n",
    "    '''\n",
    "    def __init__(self, eta, iterations=20, C=0.001, method='default', regularization_metric='NA',batch_size=0.1):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.method = method\n",
    "        self.regularization_metric = regularization_metric\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    def objective_function(self,w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    def _get_l1_dev(self,val):\n",
    "        if val > 0:\n",
    "            return 1\n",
    "        elif val == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    def _get_l1_dev(self,x):\n",
    "        if x>0:\n",
    "            return 1\n",
    "        elif x == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    def _get_gradient_StochasticLogisticRegression(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False)# get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        \n",
    "        return gradient\n",
    "\n",
    "    def _get_gradient_MiniBatchStochasticLogisticRegression(self,X,y):\n",
    "        Idx = np.arange(len(y))\n",
    "        np.random.shuffle(Idx)\n",
    "        sample_nbr = int(len(y)*self.batch_size)\n",
    "        X, y = X[Idx[:sample_nbr]], y[Idx[:sample_nbr]]\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def _get_gradient_HessianBinaryLogisticRegression(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "        \n",
    "    def line_search_function(self,eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew))) >0.5\n",
    "        return np.mean((y-yhat)**2)-C*np.mean(wnew**2)\n",
    "    \n",
    "    def objective_gradient(self,w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * w[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = w[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "\n",
    "        return -gradient\n",
    "\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'default':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        elif self.method == 'LineSearchLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient(Xb,y)\n",
    "\n",
    "                # do line search in gradient direction, using scipy function\n",
    "                opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "                res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                      bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                      args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                      method='bounded', # bounded optimization for speed\n",
    "                                      options=opts) # set max iterations\n",
    "\n",
    "                eta = res.x # get optimal learning rate\n",
    "                self.w_ += gradient*eta # set new function values\n",
    "        elif self.method == 'StochasticLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient_StochasticLogisticRegression(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        \n",
    "        elif self.method == 'MiniBatchStochasticLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "            \n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient_MiniBatchStochasticLogisticRegression(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "        elif self.method == 'HessianBinaryLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "            gradient = self._get_gradient_HessianBinaryLogisticRegression(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        \n",
    "        elif self.method == 'BFGSBinaryLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                                np.zeros((num_features,1)), # starting point\n",
    "                                fprime=self.objective_gradient, # gradient function\n",
    "                                args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                                gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                                maxiter=self.iters, # stopping criteria iterations\n",
    "                                disp=False)\n",
    "\n",
    "            self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.6002218227712548\n",
      "f1 score is  0.6766413347219502\n",
      "CPU times: user 3min 44s, sys: 9min 15s, total: 13min\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "blr = BinaryLogisticRegression(eta=0.1,iterations=500,C=0.001,\n",
    "                               method='MiniBatchStochasticLogisticRegression',\n",
    "                               regularization_metric='L1',\n",
    "                              batch_size=0.2)\n",
    "\n",
    "blr.fit(X,y)\n",
    "yhat = blr.predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y,yhat))\n",
    "print('f1 score is ',f1_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    '''\n",
    "    method: 'default', 'LineSearchLogisticRegression', 'StochasticLogisticRegression', \n",
    "    'MiniBatchStochasticLogisticRegression','HessianBinaryLogisticRegression', 'BFGSBinaryLogisticRegression'\n",
    "    regularization_metric: 'NA', 'L1', 'L2'\n",
    "    batch_size: float between 0 to 1, only valid when call 'MiniBatchStochasticLogisticRegression' method\n",
    "    processes: number of multiprocesses to use, default value 1\n",
    "    fit function: instead of fit, use multi_fit, multi_predict_proba, multi_predict\n",
    "    '''\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, method='default', \n",
    "                 regularization_metric='NA',batch_size=0.1,processes=1):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.method = method\n",
    "        self.regularization_metric = regularization_metric\n",
    "        self.batch_size = batch_size\n",
    "        self.processes = processes\n",
    "\n",
    "\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    def bi_fit(self,train_set):\n",
    "        X,y = train_set[0], train_set[1]\n",
    "        blr = BinaryLogisticRegression(self.eta,self.iters,self.C,\n",
    "                                           method=self.method,\n",
    "                                           regularization_metric=self.regularization_metric,\n",
    "                                          batch_size=self.batch_size)\n",
    "        blr.fit(X,y)\n",
    "        return blr\n",
    "    \n",
    "    def multi_fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "\n",
    "        train_sets = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval).astype(np.int)  # create a binary problem\n",
    "            train_sets.append([X,y_binary])\n",
    "\n",
    "        pool = mp.Pool(processes=self.processes)\n",
    "        self.classifiers_ = pool.map(self.bi_fit, train_sets)\n",
    "\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def bi_proba(self,predict_set):\n",
    "        blr, X = predict_set[0], predict_set[1]\n",
    "        return blr.predict_proba(X).reshape((len(X),1))\n",
    "    def multi_predict_proba(self,X):\n",
    "        predict_sets = [[blr,X] for blr in self.classifiers_]\n",
    "        pool = mp.Pool(processes=self.processes)\n",
    "        probs = pool.map(self.bi_proba, predict_sets)\n",
    "\n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def multi_predict(self,X):\n",
    "        return np.argmax(self.multi_predict_proba(X),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "\n",
    "X = StandardScaler().fit(X).transform(X)\n",
    "y_not_binary = ds.target # note problem is NOT binary anymore, there are three classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiClass Logistic Regression Object with coefficients:\n",
      "[[-1.6205769  -0.91822327  1.49877651 -1.69387707 -1.5660555 ]\n",
      " [-0.96180177  0.08272772 -1.29852342  0.39252249 -0.5127235 ]\n",
      " [-2.25475607  0.28670226  0.0173993   1.47324439  2.23329594]]\n",
      "Accuracy of:  0.9066666666666666\n",
      "CPU times: user 47.8 ms, sys: 121 ms, total: 169 ms\n",
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=0.1,iterations=500,C=0.001,\n",
    "                               method='MiniBatchStochasticLogisticRegression',\n",
    "                               regularization_metric='L1',\n",
    "                              batch_size=0.2,processes=2)\n",
    "lr.multi_fit(X,y_not_binary)\n",
    "print(lr)\n",
    "\n",
    "yhat = lr.multi_predict(X)\n",
    "print('Accuracy of: ',accuracy_score(y_not_binary,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
