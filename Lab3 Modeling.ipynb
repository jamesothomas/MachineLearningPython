{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import copy\n",
    "\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/jonavatar/Documents/MechineL/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['>30', 'NO', '<30'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['readmitted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "X = train.drop(['readmitted'], axis=1)\n",
    "y = train['readmitted']\n",
    "y.replace({'NO':0,'<30':1,'>30':2}, inplace = True)\n",
    "\n",
    "X_test = test.drop(['readmitted'], axis=1)\n",
    "y_test = test['readmitted']\n",
    "y_test.replace({'NO':0,'<30':1,'>30':2}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 µs, sys: 8 µs, total: 45 µs\n",
      "Wall time: 47.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class BinaryLogisticRegression:\n",
    "    '''\n",
    "    method: 'default', 'LineSearchLogisticRegression', 'StochasticLogisticRegression', \n",
    "    'MiniBatchStochasticLogisticRegression','HessianBinaryLogisticRegression', 'BFGSBinaryLogisticRegression'\n",
    "    regularization_metric: 'NA', 'L1', 'L2'\n",
    "    batch_size: float between 0 to 1, only valid when call 'MiniBatchStochasticLogisticRegression' method\n",
    "    '''\n",
    "    def __init__(self, eta, iterations=20, C=0.001, method='default', regularization_metric='NA',batch_size=0.1):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.method = method\n",
    "        self.regularization_metric = regularization_metric\n",
    "        self.batch_size = batch_size\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    def objective_function(self,w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + C*sum(w**2) #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    def _get_l1_dev(self,val):\n",
    "        if val > 0:\n",
    "            return 1\n",
    "        elif val == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    def _get_l1_dev(self,x):\n",
    "        if x>0:\n",
    "            return 1\n",
    "        elif x == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        return gradient\n",
    "    \n",
    "    def _get_gradient_StochasticLogisticRegression(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False)# get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        \n",
    "        return gradient\n",
    "\n",
    "    def _get_gradient_MiniBatchStochasticLogisticRegression(self,X,y):\n",
    "        Idx = np.arange(len(y))\n",
    "        np.random.shuffle(Idx)\n",
    "        sample_nbr = int(len(y)*self.batch_size)\n",
    "        X, y = X[Idx[:sample_nbr]], y[Idx[:sample_nbr]]\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def _get_gradient_HessianBinaryLogisticRegression(self,X,y):\n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = self.w_[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "        \n",
    "    def line_search_function(self,eta,X,y,w,grad,C):\n",
    "        wnew = w + grad*eta\n",
    "        yhat = (1/(1+np.exp(-X @ wnew))) >0.5\n",
    "        return np.mean((y-yhat)**2)-C*np.mean(wnew**2)\n",
    "    \n",
    "    def objective_gradient(self,w,X,y,C):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        if self.regularization_metric == 'L2':\n",
    "            gradient[1:] += -2 * w[1:] * self.C\n",
    "        elif self.regularization_metric == 'L1':\n",
    "            tmp = w[1:].copy()\n",
    "            gradient[1:] += -np.vectorize(self._get_l1_dev)(tmp) * self.C\n",
    "\n",
    "\n",
    "        return -gradient\n",
    "\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.method == 'default':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        elif self.method == 'LineSearchLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient(Xb,y)\n",
    "\n",
    "                # do line search in gradient direction, using scipy function\n",
    "                opts = {'maxiter':self.iters/20} # unclear exactly what this should be\n",
    "                res = minimize_scalar(self.line_search_function, # objective function to optimize\n",
    "                                      bounds=(self.eta/1000,self.eta*10), #bounds to optimize\n",
    "                                      args=(Xb,y,self.w_,gradient,self.C), # additional argument for objective function\n",
    "                                      method='bounded', # bounded optimization for speed\n",
    "                                      options=opts) # set max iterations\n",
    "\n",
    "                eta = res.x # get optimal learning rate\n",
    "                self.w_ += gradient*eta # set new function values\n",
    "        elif self.method == 'StochasticLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient_StochasticLogisticRegression(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        \n",
    "        elif self.method == 'MiniBatchStochasticLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "            \n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "\n",
    "            # for as many as the max iterations\n",
    "            for _ in range(self.iters):\n",
    "                gradient = self._get_gradient_MiniBatchStochasticLogisticRegression(Xb,y)\n",
    "                self.w_ += gradient*self.eta # multiply by learning rate \n",
    "\n",
    "        elif self.method == 'HessianBinaryLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "            gradient = self._get_gradient_HessianBinaryLogisticRegression(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "        \n",
    "        elif self.method == 'BFGSBinaryLogisticRegression':\n",
    "            Xb = self._add_bias(X) # add bias term\n",
    "            num_samples, num_features = Xb.shape\n",
    "\n",
    "            self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                                np.zeros((num_features,1)), # starting point\n",
    "                                fprime=self.objective_gradient, # gradient function\n",
    "                                args=(Xb,y,self.C), # extra args for gradient and objective function\n",
    "                                gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                                maxiter=self.iters, # stopping criteria iterations\n",
    "                                disp=False)\n",
    "\n",
    "            self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    '''\n",
    "    method: 'default', 'LineSearchLogisticRegression', 'StochasticLogisticRegression', \n",
    "    'MiniBatchStochasticLogisticRegression','HessianBinaryLogisticRegression', 'BFGSBinaryLogisticRegression'\n",
    "    regularization_metric: 'NA', 'L1', 'L2'\n",
    "    batch_size: float between 0 to 1, only valid when call 'MiniBatchStochasticLogisticRegression' method\n",
    "    processes: number of multiprocesses to use, default value 1\n",
    "    fit function: instead of fit, use multi_fit, multi_predict_proba, multi_predict\n",
    "    '''\n",
    "    def __init__(self, eta, iterations=20, C=0.0001, method='default', \n",
    "                 regularization_metric='NA',batch_size=0.1,processes=1):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.classifiers_ = []\n",
    "        self.method = method\n",
    "        self.regularization_metric = regularization_metric\n",
    "        self.batch_size = batch_size\n",
    "        self.processes = processes\n",
    "\n",
    "\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    def bi_fit(self,train_set):\n",
    "        X,y = train_set[0], train_set[1]\n",
    "        blr = BinaryLogisticRegression(self.eta,self.iters,self.C,\n",
    "                                           method=self.method,\n",
    "                                           regularization_metric=self.regularization_metric,\n",
    "                                          batch_size=self.batch_size)\n",
    "        blr.fit(X,y)\n",
    "        return blr\n",
    "    def multi_fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "\n",
    "        train_sets = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = (y==yval).astype(np.int)  # create a binary problem\n",
    "            train_sets.append([X,y_binary])\n",
    "\n",
    "        pool = mp.Pool(processes=self.processes)\n",
    "        self.classifiers_ = pool.map(self.bi_fit, train_sets)\n",
    "\n",
    "\n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def bi_proba(self,predict_set):\n",
    "        blr, X = predict_set[0], predict_set[1]\n",
    "        return blr.predict_proba(X).reshape((len(X),1))\n",
    "    def multi_predict_proba(self,X):\n",
    "        predict_sets = [[blr,X] for blr in self.classifiers_]\n",
    "        pool = mp.Pool(processes=self.processes)\n",
    "        probs = pool.map(self.bi_proba, predict_sets)\n",
    "\n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def multi_predict(self,X):\n",
    "        return np.argmax(self.multi_predict_proba(X),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lr = MultiClassLogisticRegression(eta=0.1,iterations=500,C=0.001,\n",
    "#                                method='MiniBatchStochasticLogisticRegression',\n",
    "#                                regularization_metric='L1',\n",
    "#                               batch_size=0.01,processes=1)\n",
    "# lr.multi_fit(X,y_test)\n",
    "# yhat=lr.multi_predict(X_test)\n",
    "# print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# reg_sklearn = LogisticRegression(multi_class='ovr').fit(X,y)\n",
    "# y_predict = reg_sklearn.predict(X_test)\n",
    "# print('Accuracy is',accuracy_score(y_test,y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-91-d84f81d1c22f>\", line 35, in bi_fit\n    blr.fit(X,y)\n  File \"<timed exec>\", line 170, in fit\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py\", line 783, in minimize_scalar\n    return _minimize_scalar_bounded(fun, bounds, args, **options)\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py\", line 1741, in _minimize_scalar_bounded\n    fx = func(x, *args)\n  File \"<timed exec>\", line 116, in line_search_function\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\", line 1071, in wrapper\n    index=left.index, name=res_name, dtype=None)\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\", line 980, in _construct_result\n    out = left._constructor(result, index=index, dtype=dtype)\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 275, in __init__\n    raise_cast_failure=True)\n  File \"/Users/jonavatar/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\", line 4165, in _sanitize_array\n    raise Exception('Data must be 1-dimensional')\nException: Data must be 1-dimensional\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-7af5abe501a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                \u001b[0mregularization_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                batch_size=0.01,processes=1)\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mreg_own\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mAccuracy_own\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_own\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy_own\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mAccuracy_own\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-d84f81d1c22f>\u001b[0m in \u001b[0;36mmulti_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbi_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         '''\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "methods=['LineSearchLogisticRegression', 'StochasticLogisticRegression', \n",
    "    'MiniBatchStochasticLogisticRegression','HessianBinaryLogisticRegression', 'BFGSBinaryLogisticRegression']\n",
    "metrics=['NA', 'L1', 'L2']\n",
    "Accuracy_best = 0\n",
    "performance = np.zeros((len(methods),len(metrics)))\n",
    "for method_id in range(len(methods)):  \n",
    "    for metric_id in range(len(metrics)):\n",
    "        for C_id in range(0,1):\n",
    "            reg_own = MultiClassLogisticRegression(eta=0.1,iterations=500,C=C_id/1000,\n",
    "                               method=methods[method_id],\n",
    "                               regularization_metric=metrics[metric_id],                  \n",
    "                               batch_size=0.01,processes=1)\n",
    "            reg_own.multi_fit(X,y)\n",
    "            Accuracy_own = accuracy_score(y_test,reg_own.predict(X_test))\n",
    "            performance[method_id][metric_id] = Accuracy_own if performance[method_id][metric_id]<Accuracy_own else performance[method_id][metric_id]\n",
    "print(performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplot.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(performance)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(methods)))\n",
    "ax.set_yticks(np.arange(len(metric)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(methods)\n",
    "ax.set_yticklabels(metric)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(matrics)):\n",
    "        text = ax.text(i, j, performance[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
